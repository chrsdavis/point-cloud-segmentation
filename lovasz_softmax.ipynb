{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGGWmLPjrYNt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from itertools import filterfalse\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def lovasz_grad(gt_sorted):\n",
        "\n",
        "\n",
        "    gts = gt_sorted.sum()\n",
        "    intersection = gts - gt_sorted.float().cumsum(0)\n",
        "    union = gts + (1 - gt_sorted).float().cumsum(0)\n",
        "    jaccard = 1. - intersection / union\n",
        "    p = len(gt_sorted)\n",
        "    if p > 1:\n",
        "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
        "    return jaccard\n",
        "\n",
        "def lovasz_softmax(probas, labels, classes = 'present', per_image = False, ignore = None):\n",
        "    \"\"\"\n",
        "      probas: [B, C, H, W] Variable, class probabilities at each prediction (softmax output of logits)\n",
        "      labels: [B, H, W] Tensor, ground truth labels (0 <= labels < C)\n",
        "      classes: 'present' for classes present in labels, 'all' for all classes, or a list of classes to average.\n",
        "      per_image: compute the loss per image instead of per batch\n",
        "      ignore: void class labels\n",
        "    \"\"\"\n",
        "\n",
        "    if per_image:\n",
        "        loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore))\n",
        "                    for prob, lab in zip(probas, labels))\n",
        "    else:\n",
        "        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore))\n",
        "    return loss\n",
        "\n",
        "def flatten_probas(probas, labels, ignore=None):\n",
        "\n",
        "\n",
        "    if ignore is None:\n",
        "        return probas.view(-1, probas.size(1)), labels.view(-1)\n",
        "    valid = labels != ignore\n",
        "    vprobas = probas.view(-1, probas.size(1))\n",
        "    vlabels = labels.view(-1)\n",
        "    return vprobas[valid], vlabels[valid]\n",
        "\n",
        "def lovasz_softmax_flat(probas, labels):\n",
        "\n",
        "\n",
        "    C = probas.size(1)\n",
        "    losses = []\n",
        "    for c in range(C):\n",
        "        fg = (labels == c).float() # foreground for class c\n",
        "        if fg.sum() == 0:\n",
        "            continue\n",
        "        if C == 1:\n",
        "            if len(probas.shape) == 1:\n",
        "                probas = probas.unsqueeze(0)\n",
        "            fg = fg.unsqueeze(1)\n",
        "        prob_fg = probas[:, c]\n",
        "        errors = (Variable(fg) - prob_fg).abs()\n",
        "        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n",
        "        fg_sorted = fg[perm]\n",
        "        losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))\n",
        "    return mean(losses)\n",
        "\n",
        "def mean(l, ignore_nan = False, empty = 0):\n",
        "\n",
        "\n",
        "    l = iter(l)\n",
        "    if ignore_nan:\n",
        "        l = filterfalse(torch.isnan, l)\n",
        "    try:\n",
        "        n = 1\n",
        "        acc = next(l)\n",
        "    except StopIteration:\n",
        "        if empty == 'raise':\n",
        "            raise ValueError(\"Empty mean\")\n",
        "        return empty\n",
        "    for n, v in enumerate(l, 2):\n",
        "        acc += v\n",
        "    if n == 1:\n",
        "        return acc\n",
        "    return acc / n"
      ]
    }
  ]
}